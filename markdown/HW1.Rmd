---
title: 'HW #1 (STAT 542, Fall 2017)'
author: "Joshua Loyal"
date: "September 7, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)

library(knitr)
library(MASS)
```

```{r echo = FALSE}
read_chunk('../R/linalg.R')
```

## Name: Joshua Loyal (jloyal2@illinois.edu)

# Question 1

a) Calculate the sample variance-covariance matrix $\hat{\Sigma}$ of $X$ (using the maximum likelihood estimator, not the unbiased version). Then calculate $\hat{\Sigma}^{-\frac{1}{2}}$.

The MLE estimator of the sample variance-covariance is given by the following formula

$$\hat{\Sigma} = \frac{1}{n} \sum_{k = 1}^{n}(X_i - \bar{X})^{T}(X_{i} - \bar{X})$$
where $X_i \in \mathbb{R}^{p \times 1}$ and $\bar{X}$ is the sample mean. This is calculated by the following function:

```{r covariance}
<<sample_covariance>>
```

Using this function we can calculate the $\hat{\Sigma}$ matrix of the given data.

```{r data generation}
set.seed(1)
P = 4
N = 200
rho = 0.5
V <- rho^abs(outer(1:P, 1:P, "-"))
X = as.matrix(mvrnorm(N, mu=rep(0,P), Sigma=V))
beta = as.matrix(c(1, 1, 0.5, 0.5))
Y = X %*% beta + rnorm(N)

(sigma <- sample_covariance(X))
```

To find the matrix square root we can use the spectral decomposition of $\Sigma$ (since covariance matrices are positive semi-definite we know it is diagonalizable and its eigen-values are positive, so the following argument is valid):

$$ \Sigma = Q \Lambda Q^{-1} \Rightarrow \Sigma^{\frac{1}{2}} = Q \sqrt{\Lambda} Q^{-1}$$
where $Q$ is the matrix eigen-vectors of $\Sigma$ and $\Lambda$ is a matrix with the eigenvalues on the diagonal. Once we have $\Sigma^{\frac{1}{2}}$ we can invert it to get $\Sigma^{-\frac{1}{2}}$. In R:

```{r matrix_root}
<<matrix_sqrt>>

solve(matrix_sqrt(sigma, symmetric = TRUE))
```

b) We want to perform a 5-NN estimation on the target point $x = (0.5, 0.5, 0.5, 0.5)^{T}$. To do this, lets first write a function `mydist <- function(x1, x2)`
that will return the Euclidean distance between any two vectors `x1` and `x2`. Calculate the distance from all sample points to the target point x and output the row numbers of the closest 5 subjects. Use their $Y$ values to obtain the 5-NN estimation at the target point.

First we need to write a function that calculates the euclidean distance between any two vectors:

```{r mydist}

#' Euclidean Distance
#'
#' Calculates the euclidean distance between two vectors
#' \code{x1} and \code{x2}.
#'
#' @param x1,x2 numeric vectors.
#' @returns the euclidean distance between the inputs.
mydist <- function(x1, x2) {
  # sanity check that x1 and x2 have the same number of features.
  if (length(x1) != length(x2))
    stop("`x1` and `x2` must be the same length.")
  
  sqrt(sum((x1 - x2)^2))
}
```

Then to find the k-nearest neighbors to a vector $y$ we need a function the calculates the
distance between $y$ and all points in a data matrix $X$, and then returns the $k$ smallest elements.

```{r nearest_neighbors}
#' Nearest Neighbors
#'
#' Find the k nearest neighbors from a point \code{x_query}
#' and all rows in a dataset \code{X}.
#'
#' @param X Feature matrix of shape [n_samples, n_features].
#' @param x_query A single vector of shape [n_features]. 
#' @param k The number of nearest neighbors to compute.
#' @param dist_func A function that defines a valid distance.
#'   The default is euclidean distance.
#' @param ... These parameters are passed into \code{dist_func}.
#' @returns The indices of the k-nearest neighbors.
nearest_neighbors <- function(X, x_query, k, dist_func = mydist, ...) {
  # If k > the number of points they are
  # all nearest-neighbors
  if (k >= nrow(X))
    return(1:nrow(X))
 
  # bind distance function to the y vector
  partial_dist <- function(x) {
    dist_func(x, x_query, ...)
  }
  
  # row-wise distances 
  distances <- apply(X, 1, partial_dist)
  
  # kth largest values
  max_value <- sort(distances, partial = k)[k]
  
  # indices that are less than or equal to the kth smallest value
  # are the nearest-neighbors
  which(distances <= max_value)[1:k]
}
```

We can use this function to find the distance between $x = (0.5, 0.5, 0.5, 0.5)^T$ and $X$.

```{r 5-NN}
x <- c(0.5, 0.5, 0.5, 0.5)
nearest_neighbors(X, x_query = x, k = 5)
```

Finally, we can make predictions by taking the sample average of the k-nearest neighbors.

```{r predict_knn}
#' K-Nearest Neighbor Predictions
#'
#' Make predictions on a data point \code{x_query}
#' based on the k-nearest neighbors in the dataset 
#' \code{X}. Predictions are made by averaging the
#' \code{Y} values of the k-neighest neighbors.
#'
#' @param X dataset of shape [n_samples, n_features]
#' @param Y target vector of shape [n_samples]
#' @param x_query vector of shape [n_features] to make predictions on.
#' @param k number of nearest neighbors to use for predictions.
#' @param dist_func function used to calculate the distance between two points.
#' @param ... These arguments are passed into \code{dist_func}.
#' @returns A single prediction for \code{x_query}.
predict_knn <- function(X, Y, x_query, k, dist_func = mydist, ...) {
  neighbors <- nearest_neighbors(X, x_query = x_query, k = k, dist_func = dist_func, ...)
  mean(Y[neighbors])
}
```

For the point $x = (0.5, 0.5, 0.5, 0.5)^T$ we have

```{r 1.b answer}
predict_knn(X, Y, x_query = x, k = 5)
```

c) Write another function `mydist2 <- function(x1, x2, s)` that returns the Mahalanobis distance between any two vectors `x1` and `x2` using any covariance matrix `s`. Redo the steps in b) to find the 5-NN estimation based on this Mahalanobis distance with `s` $= \hat{\Sigma}$.

The Mahalanobis distance is defined as

$$d(x, y) = \sqrt{(x - y)^T \Sigma^{-1} (x - y)}$$
where $\Sigma$ is the covariance matrix. 

```{r mahalanobis}

#' Mahalanobis Distance
#'
#' Calculates teh mahalanobis distance between two vectors
#' \code{x1} and \code{x2} using a arbitrary covariance
#' matrix \code{s}.
#'
#' @param x1,x2 numeric vectors.
#' @param s covariance matrix.
#' @returns the euclidean distance between the inputs.
mydist2 <- function(x1, x2, s) {
  if (length(x1) != length(x2))
    stop('`x1` and `x2` must have the same length.')
  if (nrow(s) != length(x1) || ncol(s) != length(x2))
    stop('`s` must be square and nrow = ncol = lengh(xi)')
  
  diff <- matrix(x1 - x2, nrow = 1)
  as.numeric(sqrt(diff %*% solve(s) %*% t(diff)))
}
```

Repeating 1.b with the new distance function and the $\Sigma$ matrix calculated in 1.a:

```{r 1.c answer}
predict_knn(X, Y, x_query = x, k = 5, dist_func = mydist2, s = sigma)
```

d) Which estimator seems to perform better? Can you give an explanation?