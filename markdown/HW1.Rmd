---
title: 'HW #1 (STAT 542, Fall 2017)'
author: "Joshua Loyal"
date: "September 7, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)

library(knitr)
library(MASS)
```

```{r echo = FALSE}
read_chunk('../R/linalg.R')
read_chunk('../R/neighbors.R')
```

## Name: Joshua Loyal (jloyal2@illinois.edu)

# Question 1

a) Calculate the sample variance-covariance matrix $\hat{\Sigma}$ of $X$ (using the maximum likelihood estimator, not the unbiased version). Then calculate $\hat{\Sigma}^{-\frac{1}{2}}$.

The MLE estimator of the sample variance-covariance is given by the following formula

$$\hat{\Sigma} = \frac{1}{n} \sum_{k = 1}^{n}(X_i - \bar{X})^{T}(X_{i} - \bar{X})$$
where $X_i \in \mathbb{R}^{p \times 1}$ and $\bar{X}$ is the sample mean. This is calculated by the following function:

```{r covariance}
<<sample_covariance>>
```

Using this function we can calculate the $\hat{\Sigma}$ matrix of the given data.

```{r data generation}
set.seed(1)
P = 4
N = 200
rho = 0.5
V <- rho^abs(outer(1:P, 1:P, "-"))
X = as.matrix(mvrnorm(N, mu=rep(0,P), Sigma=V))
beta = as.matrix(c(1, 1, 0.5, 0.5))
Y = X %*% beta + rnorm(N)

(sigma <- sample_covariance(X))
```

To find the matrix square root we can use the spectral decomposition of $\Sigma$ (since covariance matrices are positive semi-definite we know it is diagonalizable and its eigen-values are positive, so the following argument is valid):

$$ \Sigma = Q \Lambda Q^{-1} \Rightarrow \Sigma^{\frac{1}{2}} = Q \sqrt{\Lambda} Q^{-1}$$
where $Q$ is the matrix eigen-vectors of $\Sigma$ and $\Lambda$ is a matrix with the eigenvalues on the diagonal. Once we have $\Sigma^{\frac{1}{2}}$ we can invert it to get $\Sigma^{-\frac{1}{2}}$. In R:

```{r matrix_root}
<<matrix_sqrt>>

solve(matrix_sqrt(sigma, symmetric = TRUE))
```

b) We want to perform a 5-NN estimation on the target point $x = (0.5, 0.5, 0.5, 0.5)^{T}$. To do this, lets first write a function `mydist <- function(x1, x2)`
that will return the Euclidean distance between any two vectors `x1` and `x2`. Calculate the distance from all sample points to the target point x and output the row numbers of the closest 5 subjects. Use their $Y$ values to obtain the 5-NN estimation at the target point.

First we need to write a function that calculates the euclidean distance between any two vectors:

```{r mydist}
<<euclidean_distance>>
```

Then to find the k-nearest neighbors to a vector $y$ we need a function the calculates the
distance between $y$ and all points in a data matrix $X$, and then returns the $k$ smallest elements.

```{r nearest_neighbors_func}
<<nearest_neighbors>>
```

We can use this function to find the distance between $x = (0.5, 0.5, 0.5, 0.5)^T$ and $X$.

```{r 5-NN}
x <- c(0.5, 0.5, 0.5, 0.5)
nearest_neighbors(X, x_query = x, k = 5)
```

Finally, we can make predictions by taking the sample average of the k-nearest neighbors.

```{r predict_nearest_neighbor}
<<predict_knn>>
```

For the point $x = (0.5, 0.5, 0.5, 0.5)^T$ we have

```{r 1.b answer}
predict_knn(X, Y, x_query = x, k = 5)
```

c) Write another function `mydist2 <- function(x1, x2, s)` that returns the Mahalanobis distance between any two vectors `x1` and `x2` using any covariance matrix `s`. Redo the steps in b) to find the 5-NN estimation based on this Mahalanobis distance with `s` $= \hat{\Sigma}$.

The Mahalanobis distance is defined as

$$d(x, y) = \sqrt{(x - y)^T \Sigma^{-1} (x - y)}$$
where $\Sigma$ is the covariance matrix. 

```{r mahalanobis}
<<mahalanobis_distance>>
```

Repeating 1.b with the new distance function and the $\Sigma$ matrix calculated in 1.a:

```{r 1.c answer}
predict_knn(X, Y, x_query = x, k = 5, dist_func = mydist2, s = sigma)
```

d) Which estimator seems to perform better? Can you give an explanation?